{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Sonar_Data_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibtehaaj/Deep-Learning/blob/master/Sonar_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfMMyiOsuF0e",
        "colab_type": "code",
        "outputId": "f7c36616-bf8e-4e94-af9b-26332967d26d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# if tf.__version__ < \"2.0.0\":\n",
        "#   !pip install --upgrade tensorflow_gpu==2.0\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI8DpiPA3eQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()  # For easy reset of notebook state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJct6IlyuF0o",
        "colab_type": "code",
        "outputId": "dc1cc416-72a3-4cb9-df9d-e3afcc9b2406",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import pandas as pd\n",
        "import io"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-63ab0231-4f1e-4a2c-abc2-b303717d5828\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-63ab0231-4f1e-4a2c-abc2-b303717d5828\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sonar.csv to sonar.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7bdhQX10sKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv(\"sonar.csv\" ,header =None)\n",
        "dataset.sample()\n",
        "data = dataset.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEY9mUjMuF0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = data[:, 0:60].astype(float)\n",
        "y = data[:, 60]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ABfgnGZuF0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv(\"sonar.csv\" ,header =None)\n",
        "dataset = dataset.sample(frac=1)\n",
        "data = dataset.values\n",
        "# separating the last column of dataset which contains labels\n",
        "x = data[:, 0:60].astype(float)  \n",
        "y = data[:, 60]\n",
        "# encode class values as integers(0,1)\n",
        "le = LabelEncoder()\n",
        "encoded_Y = le.fit_transform(y)\n",
        "encoded_y_train = tf.keras.utils.to_categorical(encoded_Y)\n",
        "# slicing data into training and testing datasets\n",
        "# 60% training,30% testing and 10% validating\n",
        "x_train = x[:128,:]\n",
        "x_test = x[128:,:]\n",
        "y_train = encoded_y_train[:128]\n",
        "y_test = encoded_y_train[128:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcFfSe8zuF1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(16,activation='relu',input_dim=60),\n",
        "    # tf.keras.layers.Dense(30,activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91H7oEYGFUE_",
        "colab_type": "code",
        "outputId": "2c4515e4-d55a-49e7-f03a-1d587215adfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = build_model()\n",
        "model.fit(x_train,y_train,epochs=125,batch_size=60,verbose=2)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 128 samples\n",
            "Epoch 1/125\n",
            "128/128 - 1s - loss: 0.7185 - accuracy: 0.4844\n",
            "Epoch 2/125\n",
            "128/128 - 0s - loss: 0.7021 - accuracy: 0.5000\n",
            "Epoch 3/125\n",
            "128/128 - 0s - loss: 0.6932 - accuracy: 0.5039\n",
            "Epoch 4/125\n",
            "128/128 - 0s - loss: 0.6885 - accuracy: 0.5117\n",
            "Epoch 5/125\n",
            "128/128 - 0s - loss: 0.6825 - accuracy: 0.5195\n",
            "Epoch 6/125\n",
            "128/128 - 0s - loss: 0.6768 - accuracy: 0.5664\n",
            "Epoch 7/125\n",
            "128/128 - 0s - loss: 0.6746 - accuracy: 0.5977\n",
            "Epoch 8/125\n",
            "128/128 - 0s - loss: 0.6723 - accuracy: 0.5977\n",
            "Epoch 9/125\n",
            "128/128 - 0s - loss: 0.6696 - accuracy: 0.6602\n",
            "Epoch 10/125\n",
            "128/128 - 0s - loss: 0.6668 - accuracy: 0.6758\n",
            "Epoch 11/125\n",
            "128/128 - 0s - loss: 0.6659 - accuracy: 0.6523\n",
            "Epoch 12/125\n",
            "128/128 - 0s - loss: 0.6629 - accuracy: 0.6758\n",
            "Epoch 13/125\n",
            "128/128 - 0s - loss: 0.6609 - accuracy: 0.6992\n",
            "Epoch 14/125\n",
            "128/128 - 0s - loss: 0.6591 - accuracy: 0.7109\n",
            "Epoch 15/125\n",
            "128/128 - 0s - loss: 0.6571 - accuracy: 0.7227\n",
            "Epoch 16/125\n",
            "128/128 - 0s - loss: 0.6545 - accuracy: 0.7070\n",
            "Epoch 17/125\n",
            "128/128 - 0s - loss: 0.6527 - accuracy: 0.7148\n",
            "Epoch 18/125\n",
            "128/128 - 0s - loss: 0.6502 - accuracy: 0.7070\n",
            "Epoch 19/125\n",
            "128/128 - 0s - loss: 0.6480 - accuracy: 0.7305\n",
            "Epoch 20/125\n",
            "128/128 - 0s - loss: 0.6468 - accuracy: 0.7305\n",
            "Epoch 21/125\n",
            "128/128 - 0s - loss: 0.6438 - accuracy: 0.7188\n",
            "Epoch 22/125\n",
            "128/128 - 0s - loss: 0.6423 - accuracy: 0.7266\n",
            "Epoch 23/125\n",
            "128/128 - 0s - loss: 0.6400 - accuracy: 0.7383\n",
            "Epoch 24/125\n",
            "128/128 - 0s - loss: 0.6380 - accuracy: 0.7500\n",
            "Epoch 25/125\n",
            "128/128 - 0s - loss: 0.6356 - accuracy: 0.7305\n",
            "Epoch 26/125\n",
            "128/128 - 0s - loss: 0.6338 - accuracy: 0.7383\n",
            "Epoch 27/125\n",
            "128/128 - 0s - loss: 0.6321 - accuracy: 0.7070\n",
            "Epoch 28/125\n",
            "128/128 - 0s - loss: 0.6299 - accuracy: 0.7305\n",
            "Epoch 29/125\n",
            "128/128 - 0s - loss: 0.6281 - accuracy: 0.6992\n",
            "Epoch 30/125\n",
            "128/128 - 0s - loss: 0.6267 - accuracy: 0.6953\n",
            "Epoch 31/125\n",
            "128/128 - 0s - loss: 0.6245 - accuracy: 0.7500\n",
            "Epoch 32/125\n",
            "128/128 - 0s - loss: 0.6230 - accuracy: 0.7422\n",
            "Epoch 33/125\n",
            "128/128 - 0s - loss: 0.6214 - accuracy: 0.7227\n",
            "Epoch 34/125\n",
            "128/128 - 0s - loss: 0.6192 - accuracy: 0.7227\n",
            "Epoch 35/125\n",
            "128/128 - 0s - loss: 0.6166 - accuracy: 0.7422\n",
            "Epoch 36/125\n",
            "128/128 - 0s - loss: 0.6145 - accuracy: 0.7734\n",
            "Epoch 37/125\n",
            "128/128 - 0s - loss: 0.6119 - accuracy: 0.7773\n",
            "Epoch 38/125\n",
            "128/128 - 0s - loss: 0.6096 - accuracy: 0.7656\n",
            "Epoch 39/125\n",
            "128/128 - 0s - loss: 0.6080 - accuracy: 0.7812\n",
            "Epoch 40/125\n",
            "128/128 - 0s - loss: 0.6057 - accuracy: 0.7500\n",
            "Epoch 41/125\n",
            "128/128 - 0s - loss: 0.6037 - accuracy: 0.7461\n",
            "Epoch 42/125\n",
            "128/128 - 0s - loss: 0.6021 - accuracy: 0.7461\n",
            "Epoch 43/125\n",
            "128/128 - 0s - loss: 0.6011 - accuracy: 0.7344\n",
            "Epoch 44/125\n",
            "128/128 - 0s - loss: 0.5981 - accuracy: 0.7422\n",
            "Epoch 45/125\n",
            "128/128 - 0s - loss: 0.5963 - accuracy: 0.7500\n",
            "Epoch 46/125\n",
            "128/128 - 0s - loss: 0.5927 - accuracy: 0.7617\n",
            "Epoch 47/125\n",
            "128/128 - 0s - loss: 0.5919 - accuracy: 0.7852\n",
            "Epoch 48/125\n",
            "128/128 - 0s - loss: 0.5893 - accuracy: 0.7812\n",
            "Epoch 49/125\n",
            "128/128 - 0s - loss: 0.5875 - accuracy: 0.7539\n",
            "Epoch 50/125\n",
            "128/128 - 0s - loss: 0.5854 - accuracy: 0.7539\n",
            "Epoch 51/125\n",
            "128/128 - 0s - loss: 0.5838 - accuracy: 0.7695\n",
            "Epoch 52/125\n",
            "128/128 - 0s - loss: 0.5814 - accuracy: 0.7617\n",
            "Epoch 53/125\n",
            "128/128 - 0s - loss: 0.5789 - accuracy: 0.7656\n",
            "Epoch 54/125\n",
            "128/128 - 0s - loss: 0.5767 - accuracy: 0.7812\n",
            "Epoch 55/125\n",
            "128/128 - 0s - loss: 0.5742 - accuracy: 0.7891\n",
            "Epoch 56/125\n",
            "128/128 - 0s - loss: 0.5723 - accuracy: 0.7773\n",
            "Epoch 57/125\n",
            "128/128 - 0s - loss: 0.5700 - accuracy: 0.7656\n",
            "Epoch 58/125\n",
            "128/128 - 0s - loss: 0.5686 - accuracy: 0.7656\n",
            "Epoch 59/125\n",
            "128/128 - 0s - loss: 0.5659 - accuracy: 0.7852\n",
            "Epoch 60/125\n",
            "128/128 - 0s - loss: 0.5625 - accuracy: 0.7773\n",
            "Epoch 61/125\n",
            "128/128 - 0s - loss: 0.5616 - accuracy: 0.8047\n",
            "Epoch 62/125\n",
            "128/128 - 0s - loss: 0.5586 - accuracy: 0.8086\n",
            "Epoch 63/125\n",
            "128/128 - 0s - loss: 0.5565 - accuracy: 0.7812\n",
            "Epoch 64/125\n",
            "128/128 - 0s - loss: 0.5550 - accuracy: 0.7969\n",
            "Epoch 65/125\n",
            "128/128 - 0s - loss: 0.5539 - accuracy: 0.8086\n",
            "Epoch 66/125\n",
            "128/128 - 0s - loss: 0.5518 - accuracy: 0.8047\n",
            "Epoch 67/125\n",
            "128/128 - 0s - loss: 0.5494 - accuracy: 0.8125\n",
            "Epoch 68/125\n",
            "128/128 - 0s - loss: 0.5484 - accuracy: 0.7969\n",
            "Epoch 69/125\n",
            "128/128 - 0s - loss: 0.5478 - accuracy: 0.7773\n",
            "Epoch 70/125\n",
            "128/128 - 0s - loss: 0.5453 - accuracy: 0.8125\n",
            "Epoch 71/125\n",
            "128/128 - 0s - loss: 0.5428 - accuracy: 0.8164\n",
            "Epoch 72/125\n",
            "128/128 - 0s - loss: 0.5409 - accuracy: 0.8086\n",
            "Epoch 73/125\n",
            "128/128 - 0s - loss: 0.5391 - accuracy: 0.8164\n",
            "Epoch 74/125\n",
            "128/128 - 0s - loss: 0.5376 - accuracy: 0.8125\n",
            "Epoch 75/125\n",
            "128/128 - 0s - loss: 0.5353 - accuracy: 0.8086\n",
            "Epoch 76/125\n",
            "128/128 - 0s - loss: 0.5343 - accuracy: 0.8125\n",
            "Epoch 77/125\n",
            "128/128 - 0s - loss: 0.5326 - accuracy: 0.8125\n",
            "Epoch 78/125\n",
            "128/128 - 0s - loss: 0.5317 - accuracy: 0.7930\n",
            "Epoch 79/125\n",
            "128/128 - 0s - loss: 0.5297 - accuracy: 0.8086\n",
            "Epoch 80/125\n",
            "128/128 - 0s - loss: 0.5270 - accuracy: 0.8125\n",
            "Epoch 81/125\n",
            "128/128 - 0s - loss: 0.5253 - accuracy: 0.8125\n",
            "Epoch 82/125\n",
            "128/128 - 0s - loss: 0.5235 - accuracy: 0.8125\n",
            "Epoch 83/125\n",
            "128/128 - 0s - loss: 0.5228 - accuracy: 0.7930\n",
            "Epoch 84/125\n",
            "128/128 - 0s - loss: 0.5205 - accuracy: 0.8125\n",
            "Epoch 85/125\n",
            "128/128 - 0s - loss: 0.5187 - accuracy: 0.8164\n",
            "Epoch 86/125\n",
            "128/128 - 0s - loss: 0.5170 - accuracy: 0.8164\n",
            "Epoch 87/125\n",
            "128/128 - 0s - loss: 0.5167 - accuracy: 0.8125\n",
            "Epoch 88/125\n",
            "128/128 - 0s - loss: 0.5158 - accuracy: 0.7969\n",
            "Epoch 89/125\n",
            "128/128 - 0s - loss: 0.5131 - accuracy: 0.8125\n",
            "Epoch 90/125\n",
            "128/128 - 0s - loss: 0.5114 - accuracy: 0.8164\n",
            "Epoch 91/125\n",
            "128/128 - 0s - loss: 0.5114 - accuracy: 0.8203\n",
            "Epoch 92/125\n",
            "128/128 - 0s - loss: 0.5106 - accuracy: 0.8203\n",
            "Epoch 93/125\n",
            "128/128 - 0s - loss: 0.5068 - accuracy: 0.8203\n",
            "Epoch 94/125\n",
            "128/128 - 0s - loss: 0.5084 - accuracy: 0.8164\n",
            "Epoch 95/125\n",
            "128/128 - 0s - loss: 0.5040 - accuracy: 0.8242\n",
            "Epoch 96/125\n",
            "128/128 - 0s - loss: 0.5025 - accuracy: 0.8203\n",
            "Epoch 97/125\n",
            "128/128 - 0s - loss: 0.5008 - accuracy: 0.8242\n",
            "Epoch 98/125\n",
            "128/128 - 0s - loss: 0.4992 - accuracy: 0.8242\n",
            "Epoch 99/125\n",
            "128/128 - 0s - loss: 0.4969 - accuracy: 0.8203\n",
            "Epoch 100/125\n",
            "128/128 - 0s - loss: 0.4952 - accuracy: 0.8164\n",
            "Epoch 101/125\n",
            "128/128 - 0s - loss: 0.4928 - accuracy: 0.8203\n",
            "Epoch 102/125\n",
            "128/128 - 0s - loss: 0.4909 - accuracy: 0.8242\n",
            "Epoch 103/125\n",
            "128/128 - 0s - loss: 0.4919 - accuracy: 0.8164\n",
            "Epoch 104/125\n",
            "128/128 - 0s - loss: 0.4884 - accuracy: 0.8203\n",
            "Epoch 105/125\n",
            "128/128 - 0s - loss: 0.4874 - accuracy: 0.8203\n",
            "Epoch 106/125\n",
            "128/128 - 0s - loss: 0.4891 - accuracy: 0.7930\n",
            "Epoch 107/125\n",
            "128/128 - 0s - loss: 0.4869 - accuracy: 0.7930\n",
            "Epoch 108/125\n",
            "128/128 - 0s - loss: 0.4822 - accuracy: 0.8242\n",
            "Epoch 109/125\n",
            "128/128 - 0s - loss: 0.4822 - accuracy: 0.8242\n",
            "Epoch 110/125\n",
            "128/128 - 0s - loss: 0.4806 - accuracy: 0.8281\n",
            "Epoch 111/125\n",
            "128/128 - 0s - loss: 0.4790 - accuracy: 0.8281\n",
            "Epoch 112/125\n",
            "128/128 - 0s - loss: 0.4777 - accuracy: 0.8281\n",
            "Epoch 113/125\n",
            "128/128 - 0s - loss: 0.4758 - accuracy: 0.8359\n",
            "Epoch 114/125\n",
            "128/128 - 0s - loss: 0.4795 - accuracy: 0.8242\n",
            "Epoch 115/125\n",
            "128/128 - 0s - loss: 0.4723 - accuracy: 0.8281\n",
            "Epoch 116/125\n",
            "128/128 - 0s - loss: 0.4719 - accuracy: 0.8203\n",
            "Epoch 117/125\n",
            "128/128 - 0s - loss: 0.4703 - accuracy: 0.8359\n",
            "Epoch 118/125\n",
            "128/128 - 0s - loss: 0.4703 - accuracy: 0.8359\n",
            "Epoch 119/125\n",
            "128/128 - 0s - loss: 0.4681 - accuracy: 0.8398\n",
            "Epoch 120/125\n",
            "128/128 - 0s - loss: 0.4699 - accuracy: 0.8320\n",
            "Epoch 121/125\n",
            "128/128 - 0s - loss: 0.4660 - accuracy: 0.8242\n",
            "Epoch 122/125\n",
            "128/128 - 0s - loss: 0.4647 - accuracy: 0.8281\n",
            "Epoch 123/125\n",
            "128/128 - 0s - loss: 0.4614 - accuracy: 0.8203\n",
            "Epoch 124/125\n",
            "128/128 - 0s - loss: 0.4634 - accuracy: 0.8359\n",
            "Epoch 125/125\n",
            "128/128 - 0s - loss: 0.4613 - accuracy: 0.8320\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc4930ae5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPNp5VfFHBtR",
        "colab_type": "code",
        "outputId": "e4ab9a6f-b8d1-4a1a-91d5-4c658a1e011b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(x_test,y_test,verbose=2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80/1 - 0s - loss: 0.6051 - accuracy: 0.7312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.562777066230774, 0.73125]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZwZ645qypcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#validation data\n",
        "x_val = x_train[:20]\n",
        "partial_x_train = x_train[20:]\n",
        "y_val = y_train[:20]\n",
        "partial_y_train = y_train[20:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAQqWHNmyutY",
        "colab_type": "code",
        "outputId": "f349de81-502c-4d2f-9933-c0b15157c9af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = build_model()\n",
        "history = model.fit(partial_x_train,partial_y_train,epochs=200,batch_size=60,validation_data=(x_val, y_val))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 108 samples, validate on 20 samples\n",
            "Epoch 1/200\n",
            "108/108 [==============================] - 1s 6ms/sample - loss: 0.7046 - accuracy: 0.5000 - val_loss: 0.6872 - val_accuracy: 0.5000\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 0s 174us/sample - loss: 0.6925 - accuracy: 0.4954 - val_loss: 0.6807 - val_accuracy: 0.5250\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 0s 198us/sample - loss: 0.6867 - accuracy: 0.4861 - val_loss: 0.6753 - val_accuracy: 0.5250\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 0s 205us/sample - loss: 0.6838 - accuracy: 0.4815 - val_loss: 0.6711 - val_accuracy: 0.5750\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 0s 198us/sample - loss: 0.6796 - accuracy: 0.5093 - val_loss: 0.6684 - val_accuracy: 0.5000\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 0s 206us/sample - loss: 0.6764 - accuracy: 0.5324 - val_loss: 0.6650 - val_accuracy: 0.5000\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 0s 186us/sample - loss: 0.6738 - accuracy: 0.5694 - val_loss: 0.6619 - val_accuracy: 0.5250\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 0s 189us/sample - loss: 0.6719 - accuracy: 0.5787 - val_loss: 0.6597 - val_accuracy: 0.5500\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 0s 197us/sample - loss: 0.6697 - accuracy: 0.5787 - val_loss: 0.6574 - val_accuracy: 0.6000\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 0s 178us/sample - loss: 0.6678 - accuracy: 0.5787 - val_loss: 0.6549 - val_accuracy: 0.6250\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 0s 225us/sample - loss: 0.6655 - accuracy: 0.6019 - val_loss: 0.6522 - val_accuracy: 0.6750\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 0s 204us/sample - loss: 0.6642 - accuracy: 0.6019 - val_loss: 0.6497 - val_accuracy: 0.6750\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 0s 183us/sample - loss: 0.6622 - accuracy: 0.6157 - val_loss: 0.6474 - val_accuracy: 0.6750\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 0s 196us/sample - loss: 0.6603 - accuracy: 0.6204 - val_loss: 0.6454 - val_accuracy: 0.6750\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 202us/sample - loss: 0.6587 - accuracy: 0.6250 - val_loss: 0.6433 - val_accuracy: 0.6750\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 203us/sample - loss: 0.6571 - accuracy: 0.6435 - val_loss: 0.6416 - val_accuracy: 0.6750\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 197us/sample - loss: 0.6561 - accuracy: 0.6296 - val_loss: 0.6388 - val_accuracy: 0.6750\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 177us/sample - loss: 0.6536 - accuracy: 0.6574 - val_loss: 0.6368 - val_accuracy: 0.6750\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 208us/sample - loss: 0.6520 - accuracy: 0.6481 - val_loss: 0.6339 - val_accuracy: 0.6750\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 0s 197us/sample - loss: 0.6505 - accuracy: 0.6528 - val_loss: 0.6320 - val_accuracy: 0.6750\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 205us/sample - loss: 0.6489 - accuracy: 0.6898 - val_loss: 0.6290 - val_accuracy: 0.6750\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 212us/sample - loss: 0.6479 - accuracy: 0.6713 - val_loss: 0.6277 - val_accuracy: 0.6750\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 214us/sample - loss: 0.6457 - accuracy: 0.6667 - val_loss: 0.6256 - val_accuracy: 0.6750\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 0s 198us/sample - loss: 0.6443 - accuracy: 0.6759 - val_loss: 0.6237 - val_accuracy: 0.6750\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 200us/sample - loss: 0.6428 - accuracy: 0.6806 - val_loss: 0.6209 - val_accuracy: 0.7000\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 0s 195us/sample - loss: 0.6416 - accuracy: 0.6806 - val_loss: 0.6187 - val_accuracy: 0.7000\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 209us/sample - loss: 0.6425 - accuracy: 0.6806 - val_loss: 0.6173 - val_accuracy: 0.6750\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 0s 226us/sample - loss: 0.6392 - accuracy: 0.6898 - val_loss: 0.6152 - val_accuracy: 0.7250\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 228us/sample - loss: 0.6368 - accuracy: 0.7083 - val_loss: 0.6135 - val_accuracy: 0.7000\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 0s 256us/sample - loss: 0.6365 - accuracy: 0.6944 - val_loss: 0.6113 - val_accuracy: 0.7000\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 243us/sample - loss: 0.6347 - accuracy: 0.6759 - val_loss: 0.6096 - val_accuracy: 0.7250\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 0s 202us/sample - loss: 0.6327 - accuracy: 0.6944 - val_loss: 0.6074 - val_accuracy: 0.7250\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 0s 196us/sample - loss: 0.6314 - accuracy: 0.6806 - val_loss: 0.6056 - val_accuracy: 0.7250\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 0s 268us/sample - loss: 0.6303 - accuracy: 0.6806 - val_loss: 0.6034 - val_accuracy: 0.7250\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 0s 222us/sample - loss: 0.6281 - accuracy: 0.6991 - val_loss: 0.6003 - val_accuracy: 0.7250\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 0s 189us/sample - loss: 0.6264 - accuracy: 0.6991 - val_loss: 0.5970 - val_accuracy: 0.7250\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 0s 207us/sample - loss: 0.6243 - accuracy: 0.6852 - val_loss: 0.5928 - val_accuracy: 0.7250\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 0s 229us/sample - loss: 0.6224 - accuracy: 0.6898 - val_loss: 0.5896 - val_accuracy: 0.7500\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 0s 252us/sample - loss: 0.6197 - accuracy: 0.6898 - val_loss: 0.5858 - val_accuracy: 0.7500\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 0s 209us/sample - loss: 0.6180 - accuracy: 0.6852 - val_loss: 0.5822 - val_accuracy: 0.7750\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 0s 191us/sample - loss: 0.6178 - accuracy: 0.6759 - val_loss: 0.5790 - val_accuracy: 0.8000\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 239us/sample - loss: 0.6141 - accuracy: 0.6898 - val_loss: 0.5761 - val_accuracy: 0.7750\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 0s 233us/sample - loss: 0.6117 - accuracy: 0.6991 - val_loss: 0.5733 - val_accuracy: 0.8000\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 0s 211us/sample - loss: 0.6107 - accuracy: 0.6991 - val_loss: 0.5705 - val_accuracy: 0.8000\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 212us/sample - loss: 0.6078 - accuracy: 0.6898 - val_loss: 0.5675 - val_accuracy: 0.8000\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 0s 220us/sample - loss: 0.6080 - accuracy: 0.6991 - val_loss: 0.5646 - val_accuracy: 0.8000\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 0s 255us/sample - loss: 0.6042 - accuracy: 0.6991 - val_loss: 0.5618 - val_accuracy: 0.8000\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 0s 210us/sample - loss: 0.6039 - accuracy: 0.7176 - val_loss: 0.5596 - val_accuracy: 0.8000\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 0s 214us/sample - loss: 0.6028 - accuracy: 0.7083 - val_loss: 0.5574 - val_accuracy: 0.8000\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 0s 203us/sample - loss: 0.5989 - accuracy: 0.6944 - val_loss: 0.5547 - val_accuracy: 0.8000\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 0s 200us/sample - loss: 0.5971 - accuracy: 0.7037 - val_loss: 0.5526 - val_accuracy: 0.8000\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 0s 228us/sample - loss: 0.5953 - accuracy: 0.7176 - val_loss: 0.5502 - val_accuracy: 0.8000\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 234us/sample - loss: 0.5935 - accuracy: 0.7176 - val_loss: 0.5479 - val_accuracy: 0.8000\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 240us/sample - loss: 0.5920 - accuracy: 0.7222 - val_loss: 0.5454 - val_accuracy: 0.8500\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 268us/sample - loss: 0.5901 - accuracy: 0.7315 - val_loss: 0.5428 - val_accuracy: 0.8500\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 223us/sample - loss: 0.5889 - accuracy: 0.7269 - val_loss: 0.5403 - val_accuracy: 0.8500\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 0s 216us/sample - loss: 0.5867 - accuracy: 0.7269 - val_loss: 0.5379 - val_accuracy: 0.8500\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 196us/sample - loss: 0.5846 - accuracy: 0.7361 - val_loss: 0.5354 - val_accuracy: 0.8250\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 0s 214us/sample - loss: 0.5834 - accuracy: 0.7176 - val_loss: 0.5327 - val_accuracy: 0.8250\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 0s 197us/sample - loss: 0.5823 - accuracy: 0.7454 - val_loss: 0.5302 - val_accuracy: 0.8250\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 0s 193us/sample - loss: 0.5790 - accuracy: 0.7731 - val_loss: 0.5278 - val_accuracy: 0.8250\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 0s 209us/sample - loss: 0.5776 - accuracy: 0.7500 - val_loss: 0.5255 - val_accuracy: 0.8250\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 0s 226us/sample - loss: 0.5752 - accuracy: 0.7685 - val_loss: 0.5230 - val_accuracy: 0.8250\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 0s 185us/sample - loss: 0.5746 - accuracy: 0.7870 - val_loss: 0.5208 - val_accuracy: 0.8250\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 0s 193us/sample - loss: 0.5715 - accuracy: 0.7685 - val_loss: 0.5180 - val_accuracy: 0.8250\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 0s 178us/sample - loss: 0.5703 - accuracy: 0.7731 - val_loss: 0.5155 - val_accuracy: 0.8250\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 234us/sample - loss: 0.5676 - accuracy: 0.7731 - val_loss: 0.5127 - val_accuracy: 0.8250\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 0s 199us/sample - loss: 0.5665 - accuracy: 0.7870 - val_loss: 0.5105 - val_accuracy: 0.8250\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 0s 231us/sample - loss: 0.5647 - accuracy: 0.7870 - val_loss: 0.5079 - val_accuracy: 0.8250\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 0s 254us/sample - loss: 0.5620 - accuracy: 0.7870 - val_loss: 0.5051 - val_accuracy: 0.8250\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 0s 228us/sample - loss: 0.5604 - accuracy: 0.7963 - val_loss: 0.5028 - val_accuracy: 0.8250\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 0s 184us/sample - loss: 0.5583 - accuracy: 0.7917 - val_loss: 0.4998 - val_accuracy: 0.8250\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 0s 191us/sample - loss: 0.5582 - accuracy: 0.7870 - val_loss: 0.4978 - val_accuracy: 0.8250\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 0s 210us/sample - loss: 0.5549 - accuracy: 0.8009 - val_loss: 0.4956 - val_accuracy: 0.8250\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 0s 217us/sample - loss: 0.5570 - accuracy: 0.7870 - val_loss: 0.4934 - val_accuracy: 0.8500\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 0s 227us/sample - loss: 0.5509 - accuracy: 0.7963 - val_loss: 0.4915 - val_accuracy: 0.8500\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 0s 209us/sample - loss: 0.5520 - accuracy: 0.7963 - val_loss: 0.4897 - val_accuracy: 0.8250\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 0s 242us/sample - loss: 0.5479 - accuracy: 0.8148 - val_loss: 0.4874 - val_accuracy: 0.8500\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 0s 258us/sample - loss: 0.5467 - accuracy: 0.8056 - val_loss: 0.4854 - val_accuracy: 0.8500\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 0s 188us/sample - loss: 0.5452 - accuracy: 0.8102 - val_loss: 0.4830 - val_accuracy: 0.8500\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 0s 194us/sample - loss: 0.5430 - accuracy: 0.8102 - val_loss: 0.4810 - val_accuracy: 0.8500\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 0s 193us/sample - loss: 0.5422 - accuracy: 0.8102 - val_loss: 0.4787 - val_accuracy: 0.8500\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 0s 193us/sample - loss: 0.5397 - accuracy: 0.8148 - val_loss: 0.4764 - val_accuracy: 0.8500\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 0s 226us/sample - loss: 0.5401 - accuracy: 0.8148 - val_loss: 0.4746 - val_accuracy: 0.8500\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 0s 215us/sample - loss: 0.5371 - accuracy: 0.8102 - val_loss: 0.4727 - val_accuracy: 0.8500\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 0s 223us/sample - loss: 0.5358 - accuracy: 0.8056 - val_loss: 0.4703 - val_accuracy: 0.8500\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 0s 199us/sample - loss: 0.5398 - accuracy: 0.7963 - val_loss: 0.4688 - val_accuracy: 0.8500\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 206us/sample - loss: 0.5322 - accuracy: 0.8102 - val_loss: 0.4668 - val_accuracy: 0.8500\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 204us/sample - loss: 0.5304 - accuracy: 0.8102 - val_loss: 0.4647 - val_accuracy: 0.8500\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 198us/sample - loss: 0.5290 - accuracy: 0.8148 - val_loss: 0.4626 - val_accuracy: 0.8500\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 0s 227us/sample - loss: 0.5292 - accuracy: 0.8148 - val_loss: 0.4608 - val_accuracy: 0.8500\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 0s 226us/sample - loss: 0.5265 - accuracy: 0.8194 - val_loss: 0.4587 - val_accuracy: 0.8500\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 0s 236us/sample - loss: 0.5244 - accuracy: 0.8148 - val_loss: 0.4567 - val_accuracy: 0.8500\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 0s 231us/sample - loss: 0.5228 - accuracy: 0.8102 - val_loss: 0.4545 - val_accuracy: 0.8500\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 0s 234us/sample - loss: 0.5217 - accuracy: 0.8148 - val_loss: 0.4528 - val_accuracy: 0.8500\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 0s 231us/sample - loss: 0.5198 - accuracy: 0.8194 - val_loss: 0.4507 - val_accuracy: 0.8500\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 0s 217us/sample - loss: 0.5180 - accuracy: 0.8194 - val_loss: 0.4484 - val_accuracy: 0.8500\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 0s 208us/sample - loss: 0.5200 - accuracy: 0.8056 - val_loss: 0.4469 - val_accuracy: 0.8750\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 0s 221us/sample - loss: 0.5154 - accuracy: 0.8194 - val_loss: 0.4451 - val_accuracy: 0.8500\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 0s 208us/sample - loss: 0.5169 - accuracy: 0.8056 - val_loss: 0.4435 - val_accuracy: 0.8500\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 0s 216us/sample - loss: 0.5128 - accuracy: 0.8148 - val_loss: 0.4415 - val_accuracy: 0.8500\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 0s 189us/sample - loss: 0.5153 - accuracy: 0.8148 - val_loss: 0.4402 - val_accuracy: 0.8750\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 0s 233us/sample - loss: 0.5107 - accuracy: 0.8194 - val_loss: 0.4387 - val_accuracy: 0.8500\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 0s 235us/sample - loss: 0.5088 - accuracy: 0.8287 - val_loss: 0.4366 - val_accuracy: 0.8500\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 0s 215us/sample - loss: 0.5070 - accuracy: 0.8287 - val_loss: 0.4351 - val_accuracy: 0.8500\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 209us/sample - loss: 0.5058 - accuracy: 0.8194 - val_loss: 0.4330 - val_accuracy: 0.8500\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 210us/sample - loss: 0.5051 - accuracy: 0.8148 - val_loss: 0.4316 - val_accuracy: 0.9000\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 214us/sample - loss: 0.5056 - accuracy: 0.8148 - val_loss: 0.4300 - val_accuracy: 0.8500\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 0s 203us/sample - loss: 0.5020 - accuracy: 0.8241 - val_loss: 0.4283 - val_accuracy: 0.8750\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 0s 201us/sample - loss: 0.5015 - accuracy: 0.8194 - val_loss: 0.4264 - val_accuracy: 0.8500\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 0s 194us/sample - loss: 0.5016 - accuracy: 0.8194 - val_loss: 0.4252 - val_accuracy: 0.8500\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 224us/sample - loss: 0.4980 - accuracy: 0.8241 - val_loss: 0.4232 - val_accuracy: 0.8750\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 0s 198us/sample - loss: 0.4964 - accuracy: 0.8287 - val_loss: 0.4210 - val_accuracy: 0.8750\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 0s 197us/sample - loss: 0.4987 - accuracy: 0.8241 - val_loss: 0.4198 - val_accuracy: 0.8500\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 0s 229us/sample - loss: 0.4938 - accuracy: 0.8287 - val_loss: 0.4183 - val_accuracy: 0.8750\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 0s 237us/sample - loss: 0.4942 - accuracy: 0.8148 - val_loss: 0.4169 - val_accuracy: 0.8500\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 0s 214us/sample - loss: 0.4915 - accuracy: 0.8241 - val_loss: 0.4148 - val_accuracy: 0.8750\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 0s 272us/sample - loss: 0.4926 - accuracy: 0.8194 - val_loss: 0.4133 - val_accuracy: 0.9000\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 0s 220us/sample - loss: 0.4894 - accuracy: 0.8241 - val_loss: 0.4115 - val_accuracy: 0.9000\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 0s 210us/sample - loss: 0.4877 - accuracy: 0.8241 - val_loss: 0.4104 - val_accuracy: 0.8750\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 0s 198us/sample - loss: 0.4885 - accuracy: 0.8287 - val_loss: 0.4090 - val_accuracy: 0.8750\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 0s 260us/sample - loss: 0.4851 - accuracy: 0.8241 - val_loss: 0.4071 - val_accuracy: 0.8750\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 0s 181us/sample - loss: 0.4838 - accuracy: 0.8380 - val_loss: 0.4059 - val_accuracy: 0.8750\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 0s 200us/sample - loss: 0.4845 - accuracy: 0.8287 - val_loss: 0.4043 - val_accuracy: 0.9000\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 0s 227us/sample - loss: 0.4816 - accuracy: 0.8287 - val_loss: 0.4022 - val_accuracy: 0.9000\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 0s 197us/sample - loss: 0.4822 - accuracy: 0.8287 - val_loss: 0.4011 - val_accuracy: 0.9000\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 0s 301us/sample - loss: 0.4789 - accuracy: 0.8333 - val_loss: 0.3995 - val_accuracy: 0.9000\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 0s 207us/sample - loss: 0.4775 - accuracy: 0.8241 - val_loss: 0.3977 - val_accuracy: 0.9000\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 0s 183us/sample - loss: 0.4762 - accuracy: 0.8287 - val_loss: 0.3961 - val_accuracy: 0.8750\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 0s 175us/sample - loss: 0.4750 - accuracy: 0.8333 - val_loss: 0.3939 - val_accuracy: 0.9000\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 0s 193us/sample - loss: 0.4779 - accuracy: 0.8148 - val_loss: 0.3930 - val_accuracy: 0.9000\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 0s 193us/sample - loss: 0.4730 - accuracy: 0.8333 - val_loss: 0.3910 - val_accuracy: 0.9000\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 0s 222us/sample - loss: 0.4719 - accuracy: 0.8287 - val_loss: 0.3902 - val_accuracy: 0.9000\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 0s 180us/sample - loss: 0.4707 - accuracy: 0.8287 - val_loss: 0.3888 - val_accuracy: 0.9000\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 0s 188us/sample - loss: 0.4706 - accuracy: 0.8287 - val_loss: 0.3878 - val_accuracy: 0.8750\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 0s 223us/sample - loss: 0.4681 - accuracy: 0.8380 - val_loss: 0.3865 - val_accuracy: 0.8750\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 206us/sample - loss: 0.4669 - accuracy: 0.8380 - val_loss: 0.3845 - val_accuracy: 0.9000\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 0s 236us/sample - loss: 0.4658 - accuracy: 0.8380 - val_loss: 0.3832 - val_accuracy: 0.8750\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 0s 211us/sample - loss: 0.4662 - accuracy: 0.8472 - val_loss: 0.3825 - val_accuracy: 0.8500\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 0s 233us/sample - loss: 0.4630 - accuracy: 0.8287 - val_loss: 0.3804 - val_accuracy: 0.9000\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 0s 192us/sample - loss: 0.4619 - accuracy: 0.8380 - val_loss: 0.3793 - val_accuracy: 0.8750\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 0s 204us/sample - loss: 0.4605 - accuracy: 0.8333 - val_loss: 0.3774 - val_accuracy: 0.9000\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 204us/sample - loss: 0.4602 - accuracy: 0.8333 - val_loss: 0.3762 - val_accuracy: 0.8500\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 0s 276us/sample - loss: 0.4589 - accuracy: 0.8333 - val_loss: 0.3750 - val_accuracy: 0.8500\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 0s 221us/sample - loss: 0.4571 - accuracy: 0.8333 - val_loss: 0.3723 - val_accuracy: 0.9000\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 0s 223us/sample - loss: 0.4572 - accuracy: 0.8380 - val_loss: 0.3707 - val_accuracy: 0.9250\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 0s 192us/sample - loss: 0.4546 - accuracy: 0.8426 - val_loss: 0.3702 - val_accuracy: 0.9000\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 0s 188us/sample - loss: 0.4530 - accuracy: 0.8333 - val_loss: 0.3684 - val_accuracy: 0.9000\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 0s 232us/sample - loss: 0.4531 - accuracy: 0.8380 - val_loss: 0.3682 - val_accuracy: 0.8500\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 0s 219us/sample - loss: 0.4519 - accuracy: 0.8380 - val_loss: 0.3670 - val_accuracy: 0.8500\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 0s 213us/sample - loss: 0.4500 - accuracy: 0.8241 - val_loss: 0.3653 - val_accuracy: 0.8750\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 0s 220us/sample - loss: 0.4520 - accuracy: 0.8380 - val_loss: 0.3647 - val_accuracy: 0.8500\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 0s 216us/sample - loss: 0.4475 - accuracy: 0.8380 - val_loss: 0.3633 - val_accuracy: 0.8750\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 0s 201us/sample - loss: 0.4477 - accuracy: 0.8380 - val_loss: 0.3624 - val_accuracy: 0.8500\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 0s 223us/sample - loss: 0.4458 - accuracy: 0.8426 - val_loss: 0.3600 - val_accuracy: 0.8750\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 0s 211us/sample - loss: 0.4445 - accuracy: 0.8380 - val_loss: 0.3585 - val_accuracy: 0.9000\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 0s 215us/sample - loss: 0.4432 - accuracy: 0.8380 - val_loss: 0.3576 - val_accuracy: 0.8750\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 0s 283us/sample - loss: 0.4424 - accuracy: 0.8426 - val_loss: 0.3560 - val_accuracy: 0.9000\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 0s 214us/sample - loss: 0.4418 - accuracy: 0.8426 - val_loss: 0.3556 - val_accuracy: 0.8750\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 0s 227us/sample - loss: 0.4398 - accuracy: 0.8380 - val_loss: 0.3537 - val_accuracy: 0.9000\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 0s 212us/sample - loss: 0.4399 - accuracy: 0.8472 - val_loss: 0.3526 - val_accuracy: 0.9000\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 0s 200us/sample - loss: 0.4383 - accuracy: 0.8426 - val_loss: 0.3525 - val_accuracy: 0.8750\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 0s 214us/sample - loss: 0.4414 - accuracy: 0.8380 - val_loss: 0.3504 - val_accuracy: 0.9000\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 0s 209us/sample - loss: 0.4356 - accuracy: 0.8426 - val_loss: 0.3497 - val_accuracy: 0.9000\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 194us/sample - loss: 0.4350 - accuracy: 0.8472 - val_loss: 0.3482 - val_accuracy: 0.9000\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 0s 217us/sample - loss: 0.4339 - accuracy: 0.8426 - val_loss: 0.3476 - val_accuracy: 0.9000\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 0s 240us/sample - loss: 0.4339 - accuracy: 0.8380 - val_loss: 0.3476 - val_accuracy: 0.8750\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 0s 239us/sample - loss: 0.4324 - accuracy: 0.8380 - val_loss: 0.3455 - val_accuracy: 0.9000\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 0s 221us/sample - loss: 0.4309 - accuracy: 0.8472 - val_loss: 0.3447 - val_accuracy: 0.9000\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 0s 184us/sample - loss: 0.4328 - accuracy: 0.8426 - val_loss: 0.3436 - val_accuracy: 0.9000\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 0s 217us/sample - loss: 0.4296 - accuracy: 0.8426 - val_loss: 0.3436 - val_accuracy: 0.8750\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 0s 227us/sample - loss: 0.4300 - accuracy: 0.8426 - val_loss: 0.3419 - val_accuracy: 0.9000\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 0s 199us/sample - loss: 0.4281 - accuracy: 0.8519 - val_loss: 0.3412 - val_accuracy: 0.9000\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 0s 185us/sample - loss: 0.4265 - accuracy: 0.8472 - val_loss: 0.3408 - val_accuracy: 0.8750\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 0s 190us/sample - loss: 0.4255 - accuracy: 0.8472 - val_loss: 0.3401 - val_accuracy: 0.9000\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 0s 193us/sample - loss: 0.4245 - accuracy: 0.8472 - val_loss: 0.3394 - val_accuracy: 0.8750\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 0s 232us/sample - loss: 0.4267 - accuracy: 0.8380 - val_loss: 0.3373 - val_accuracy: 0.9000\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 0s 211us/sample - loss: 0.4229 - accuracy: 0.8472 - val_loss: 0.3364 - val_accuracy: 0.9000\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 0s 167us/sample - loss: 0.4216 - accuracy: 0.8519 - val_loss: 0.3361 - val_accuracy: 0.9000\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 201us/sample - loss: 0.4255 - accuracy: 0.8472 - val_loss: 0.3366 - val_accuracy: 0.8750\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 0s 185us/sample - loss: 0.4200 - accuracy: 0.8380 - val_loss: 0.3351 - val_accuracy: 0.9000\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 0s 171us/sample - loss: 0.4258 - accuracy: 0.8241 - val_loss: 0.3339 - val_accuracy: 0.9000\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 0s 172us/sample - loss: 0.4185 - accuracy: 0.8519 - val_loss: 0.3336 - val_accuracy: 0.9000\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 0s 172us/sample - loss: 0.4188 - accuracy: 0.8426 - val_loss: 0.3337 - val_accuracy: 0.8750\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 0s 187us/sample - loss: 0.4224 - accuracy: 0.8472 - val_loss: 0.3318 - val_accuracy: 0.9000\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 0s 211us/sample - loss: 0.4164 - accuracy: 0.8426 - val_loss: 0.3316 - val_accuracy: 0.9000\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 0s 210us/sample - loss: 0.4175 - accuracy: 0.8472 - val_loss: 0.3306 - val_accuracy: 0.9000\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 0s 222us/sample - loss: 0.4159 - accuracy: 0.8565 - val_loss: 0.3300 - val_accuracy: 0.9000\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 0s 209us/sample - loss: 0.4150 - accuracy: 0.8519 - val_loss: 0.3306 - val_accuracy: 0.9000\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 0s 199us/sample - loss: 0.4148 - accuracy: 0.8565 - val_loss: 0.3305 - val_accuracy: 0.8750\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 0s 175us/sample - loss: 0.4127 - accuracy: 0.8472 - val_loss: 0.3293 - val_accuracy: 0.9000\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 0s 183us/sample - loss: 0.4125 - accuracy: 0.8426 - val_loss: 0.3287 - val_accuracy: 0.8750\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 0s 190us/sample - loss: 0.4144 - accuracy: 0.8472 - val_loss: 0.3266 - val_accuracy: 0.9000\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 0s 185us/sample - loss: 0.4113 - accuracy: 0.8519 - val_loss: 0.3264 - val_accuracy: 0.9000\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 0s 201us/sample - loss: 0.4110 - accuracy: 0.8426 - val_loss: 0.3257 - val_accuracy: 0.9000\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 0s 201us/sample - loss: 0.4092 - accuracy: 0.8565 - val_loss: 0.3252 - val_accuracy: 0.9000\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 253us/sample - loss: 0.4084 - accuracy: 0.8426 - val_loss: 0.3248 - val_accuracy: 0.9000\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 0s 178us/sample - loss: 0.4076 - accuracy: 0.8472 - val_loss: 0.3243 - val_accuracy: 0.9000\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 0s 210us/sample - loss: 0.4072 - accuracy: 0.8426 - val_loss: 0.3242 - val_accuracy: 0.8750\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 0s 177us/sample - loss: 0.4058 - accuracy: 0.8472 - val_loss: 0.3225 - val_accuracy: 0.9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH22XIoQTLXT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "1)A plot of learning curves shows overfitting if:\n",
        "The plot of training loss continues to decrease with experience.\n",
        "The plot of validation loss decreases to a point and begins increasing again.\n",
        "\n",
        "2)A plot of learning curves shows underfitting if:\n",
        "The training loss remains flat regardless of training.\n",
        "The training loss continues to decrease until the end of training.\n",
        "\n",
        "\n",
        "\n",
        "3)A plot of learning curves shows a good fit if:\n",
        "The plot of training loss decreases to a point of stability.\n",
        "The plot of validation loss decreases to a point of stability and has a small gap with the training loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cWeqBQOy0Qk",
        "colab_type": "code",
        "outputId": "f7988c13-87b4-4000-d6df-9cc3d442ea3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1, len(history_dict['accuracy']) + 1)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njAnJEoQxxpiQLEEYY4wJ6f8DOU9ddOGBDKsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWECOIa5y5rD",
        "colab_type": "code",
        "outputId": "1a0ac18e-fd45-44a3-9cd9-fb96da1c09ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.clf()\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "plt.plot(epochs, history_dict['accuracy'], 'bo', label='Training acc')\n",
        "plt.plot(epochs, history_dict['val_accuracy'], 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDMMocEwRGIZhFDj/Hw2CbisWNZQuAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxYCsmehFWhW",
        "colab_type": "text"
      },
      "source": [
        "# K-Fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diHyA7dp6arN",
        "colab_type": "code",
        "outputId": "dcec5c60-bb5e-43e3-8926-442990d24262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "k = 4\n",
        "num_val_samples = len(x_train) // k\n",
        "num_epochs = 100\n",
        "l = []\n",
        "a = []\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "  val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  partial_train_data = np.concatenate(\n",
        "  [x_train[:i * num_val_samples],\n",
        "  x_train[(i + 1) * num_val_samples:]],\n",
        "  axis=0)\n",
        "  partial_train_targets = np.concatenate(\n",
        "  [y_train[:i * num_val_samples],\n",
        "  y_train[(i + 1) * num_val_samples:]],\n",
        "  axis=0)\n",
        "  model = build_model()\n",
        "  model.fit(partial_train_data, partial_train_targets,\n",
        "  epochs=num_epochs, batch_size=60, verbose=2)\n",
        "  loss, acc = model.evaluate(val_data, val_targets, verbose=2)\n",
        "  l.append(loss)\n",
        "  a.append(acc)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "Train on 96 samples\n",
            "Epoch 1/100\n",
            "96/96 - 1s - loss: 0.7284 - accuracy: 0.3958\n",
            "Epoch 2/100\n",
            "96/96 - 0s - loss: 0.7169 - accuracy: 0.4479\n",
            "Epoch 3/100\n",
            "96/96 - 0s - loss: 0.7125 - accuracy: 0.4531\n",
            "Epoch 4/100\n",
            "96/96 - 0s - loss: 0.7089 - accuracy: 0.4375\n",
            "Epoch 5/100\n",
            "96/96 - 0s - loss: 0.7062 - accuracy: 0.4062\n",
            "Epoch 6/100\n",
            "96/96 - 0s - loss: 0.7038 - accuracy: 0.4062\n",
            "Epoch 7/100\n",
            "96/96 - 0s - loss: 0.7012 - accuracy: 0.4115\n",
            "Epoch 8/100\n",
            "96/96 - 0s - loss: 0.6990 - accuracy: 0.4062\n",
            "Epoch 9/100\n",
            "96/96 - 0s - loss: 0.6977 - accuracy: 0.4115\n",
            "Epoch 10/100\n",
            "96/96 - 0s - loss: 0.6955 - accuracy: 0.4323\n",
            "Epoch 11/100\n",
            "96/96 - 0s - loss: 0.6928 - accuracy: 0.4844\n",
            "Epoch 12/100\n",
            "96/96 - 0s - loss: 0.6907 - accuracy: 0.4948\n",
            "Epoch 13/100\n",
            "96/96 - 0s - loss: 0.6887 - accuracy: 0.5052\n",
            "Epoch 14/100\n",
            "96/96 - 0s - loss: 0.6873 - accuracy: 0.5365\n",
            "Epoch 15/100\n",
            "96/96 - 0s - loss: 0.6854 - accuracy: 0.5208\n",
            "Epoch 16/100\n",
            "96/96 - 0s - loss: 0.6828 - accuracy: 0.5312\n",
            "Epoch 17/100\n",
            "96/96 - 0s - loss: 0.6808 - accuracy: 0.5729\n",
            "Epoch 18/100\n",
            "96/96 - 0s - loss: 0.6786 - accuracy: 0.5729\n",
            "Epoch 19/100\n",
            "96/96 - 0s - loss: 0.6769 - accuracy: 0.5729\n",
            "Epoch 20/100\n",
            "96/96 - 0s - loss: 0.6755 - accuracy: 0.5938\n",
            "Epoch 21/100\n",
            "96/96 - 0s - loss: 0.6727 - accuracy: 0.6094\n",
            "Epoch 22/100\n",
            "96/96 - 0s - loss: 0.6712 - accuracy: 0.6302\n",
            "Epoch 23/100\n",
            "96/96 - 0s - loss: 0.6691 - accuracy: 0.6302\n",
            "Epoch 24/100\n",
            "96/96 - 0s - loss: 0.6669 - accuracy: 0.6562\n",
            "Epoch 25/100\n",
            "96/96 - 0s - loss: 0.6656 - accuracy: 0.6354\n",
            "Epoch 26/100\n",
            "96/96 - 0s - loss: 0.6631 - accuracy: 0.6510\n",
            "Epoch 27/100\n",
            "96/96 - 0s - loss: 0.6612 - accuracy: 0.6667\n",
            "Epoch 28/100\n",
            "96/96 - 0s - loss: 0.6595 - accuracy: 0.6667\n",
            "Epoch 29/100\n",
            "96/96 - 0s - loss: 0.6583 - accuracy: 0.6771\n",
            "Epoch 30/100\n",
            "96/96 - 0s - loss: 0.6561 - accuracy: 0.6927\n",
            "Epoch 31/100\n",
            "96/96 - 0s - loss: 0.6563 - accuracy: 0.6719\n",
            "Epoch 32/100\n",
            "96/96 - 0s - loss: 0.6524 - accuracy: 0.6771\n",
            "Epoch 33/100\n",
            "96/96 - 0s - loss: 0.6508 - accuracy: 0.6875\n",
            "Epoch 34/100\n",
            "96/96 - 0s - loss: 0.6491 - accuracy: 0.6667\n",
            "Epoch 35/100\n",
            "96/96 - 0s - loss: 0.6468 - accuracy: 0.6719\n",
            "Epoch 36/100\n",
            "96/96 - 0s - loss: 0.6450 - accuracy: 0.6875\n",
            "Epoch 37/100\n",
            "96/96 - 0s - loss: 0.6426 - accuracy: 0.6927\n",
            "Epoch 38/100\n",
            "96/96 - 0s - loss: 0.6403 - accuracy: 0.6979\n",
            "Epoch 39/100\n",
            "96/96 - 0s - loss: 0.6383 - accuracy: 0.7188\n",
            "Epoch 40/100\n",
            "96/96 - 0s - loss: 0.6362 - accuracy: 0.7188\n",
            "Epoch 41/100\n",
            "96/96 - 0s - loss: 0.6358 - accuracy: 0.7031\n",
            "Epoch 42/100\n",
            "96/96 - 0s - loss: 0.6335 - accuracy: 0.7448\n",
            "Epoch 43/100\n",
            "96/96 - 0s - loss: 0.6304 - accuracy: 0.7396\n",
            "Epoch 44/100\n",
            "96/96 - 0s - loss: 0.6285 - accuracy: 0.7448\n",
            "Epoch 45/100\n",
            "96/96 - 0s - loss: 0.6269 - accuracy: 0.7396\n",
            "Epoch 46/100\n",
            "96/96 - 0s - loss: 0.6253 - accuracy: 0.7552\n",
            "Epoch 47/100\n",
            "96/96 - 0s - loss: 0.6246 - accuracy: 0.7188\n",
            "Epoch 48/100\n",
            "96/96 - 0s - loss: 0.6229 - accuracy: 0.7552\n",
            "Epoch 49/100\n",
            "96/96 - 0s - loss: 0.6197 - accuracy: 0.7760\n",
            "Epoch 50/100\n",
            "96/96 - 0s - loss: 0.6190 - accuracy: 0.7448\n",
            "Epoch 51/100\n",
            "96/96 - 0s - loss: 0.6163 - accuracy: 0.7708\n",
            "Epoch 52/100\n",
            "96/96 - 0s - loss: 0.6147 - accuracy: 0.7656\n",
            "Epoch 53/100\n",
            "96/96 - 0s - loss: 0.6126 - accuracy: 0.7708\n",
            "Epoch 54/100\n",
            "96/96 - 0s - loss: 0.6112 - accuracy: 0.7656\n",
            "Epoch 55/100\n",
            "96/96 - 0s - loss: 0.6090 - accuracy: 0.7708\n",
            "Epoch 56/100\n",
            "96/96 - 0s - loss: 0.6092 - accuracy: 0.7865\n",
            "Epoch 57/100\n",
            "96/96 - 0s - loss: 0.6083 - accuracy: 0.7448\n",
            "Epoch 58/100\n",
            "96/96 - 0s - loss: 0.6040 - accuracy: 0.7760\n",
            "Epoch 59/100\n",
            "96/96 - 0s - loss: 0.6024 - accuracy: 0.7760\n",
            "Epoch 60/100\n",
            "96/96 - 0s - loss: 0.6007 - accuracy: 0.7812\n",
            "Epoch 61/100\n",
            "96/96 - 0s - loss: 0.5989 - accuracy: 0.7812\n",
            "Epoch 62/100\n",
            "96/96 - 0s - loss: 0.5972 - accuracy: 0.7708\n",
            "Epoch 63/100\n",
            "96/96 - 0s - loss: 0.5953 - accuracy: 0.7865\n",
            "Epoch 64/100\n",
            "96/96 - 0s - loss: 0.5934 - accuracy: 0.7865\n",
            "Epoch 65/100\n",
            "96/96 - 0s - loss: 0.5914 - accuracy: 0.7865\n",
            "Epoch 66/100\n",
            "96/96 - 0s - loss: 0.5909 - accuracy: 0.7812\n",
            "Epoch 67/100\n",
            "96/96 - 0s - loss: 0.5879 - accuracy: 0.7604\n",
            "Epoch 68/100\n",
            "96/96 - 0s - loss: 0.5857 - accuracy: 0.7865\n",
            "Epoch 69/100\n",
            "96/96 - 0s - loss: 0.5839 - accuracy: 0.7865\n",
            "Epoch 70/100\n",
            "96/96 - 0s - loss: 0.5827 - accuracy: 0.7812\n",
            "Epoch 71/100\n",
            "96/96 - 0s - loss: 0.5800 - accuracy: 0.8073\n",
            "Epoch 72/100\n",
            "96/96 - 0s - loss: 0.5779 - accuracy: 0.7917\n",
            "Epoch 73/100\n",
            "96/96 - 0s - loss: 0.5757 - accuracy: 0.7917\n",
            "Epoch 74/100\n",
            "96/96 - 0s - loss: 0.5744 - accuracy: 0.7917\n",
            "Epoch 75/100\n",
            "96/96 - 0s - loss: 0.5727 - accuracy: 0.8021\n",
            "Epoch 76/100\n",
            "96/96 - 0s - loss: 0.5707 - accuracy: 0.7865\n",
            "Epoch 77/100\n",
            "96/96 - 0s - loss: 0.5683 - accuracy: 0.7812\n",
            "Epoch 78/100\n",
            "96/96 - 0s - loss: 0.5666 - accuracy: 0.7760\n",
            "Epoch 79/100\n",
            "96/96 - 0s - loss: 0.5666 - accuracy: 0.7708\n",
            "Epoch 80/100\n",
            "96/96 - 0s - loss: 0.5632 - accuracy: 0.8177\n",
            "Epoch 81/100\n",
            "96/96 - 0s - loss: 0.5614 - accuracy: 0.8021\n",
            "Epoch 82/100\n",
            "96/96 - 0s - loss: 0.5593 - accuracy: 0.8125\n",
            "Epoch 83/100\n",
            "96/96 - 0s - loss: 0.5602 - accuracy: 0.7865\n",
            "Epoch 84/100\n",
            "96/96 - 0s - loss: 0.5562 - accuracy: 0.7917\n",
            "Epoch 85/100\n",
            "96/96 - 0s - loss: 0.5544 - accuracy: 0.7969\n",
            "Epoch 86/100\n",
            "96/96 - 0s - loss: 0.5525 - accuracy: 0.7969\n",
            "Epoch 87/100\n",
            "96/96 - 0s - loss: 0.5518 - accuracy: 0.8021\n",
            "Epoch 88/100\n",
            "96/96 - 0s - loss: 0.5495 - accuracy: 0.8177\n",
            "Epoch 89/100\n",
            "96/96 - 0s - loss: 0.5477 - accuracy: 0.8125\n",
            "Epoch 90/100\n",
            "96/96 - 0s - loss: 0.5464 - accuracy: 0.7917\n",
            "Epoch 91/100\n",
            "96/96 - 0s - loss: 0.5438 - accuracy: 0.8073\n",
            "Epoch 92/100\n",
            "96/96 - 0s - loss: 0.5424 - accuracy: 0.8021\n",
            "Epoch 93/100\n",
            "96/96 - 0s - loss: 0.5403 - accuracy: 0.7969\n",
            "Epoch 94/100\n",
            "96/96 - 0s - loss: 0.5394 - accuracy: 0.8073\n",
            "Epoch 95/100\n",
            "96/96 - 0s - loss: 0.5382 - accuracy: 0.7917\n",
            "Epoch 96/100\n",
            "96/96 - 0s - loss: 0.5353 - accuracy: 0.7812\n",
            "Epoch 97/100\n",
            "96/96 - 0s - loss: 0.5339 - accuracy: 0.7917\n",
            "Epoch 98/100\n",
            "96/96 - 0s - loss: 0.5322 - accuracy: 0.7969\n",
            "Epoch 99/100\n",
            "96/96 - 0s - loss: 0.5313 - accuracy: 0.7969\n",
            "Epoch 100/100\n",
            "96/96 - 0s - loss: 0.5287 - accuracy: 0.7865\n",
            "32/1 - 0s - loss: 0.5173 - accuracy: 0.8281\n",
            "processing fold # 1\n",
            "Train on 96 samples\n",
            "Epoch 1/100\n",
            "96/96 - 0s - loss: 0.7538 - accuracy: 0.5052\n",
            "Epoch 2/100\n",
            "96/96 - 0s - loss: 0.7322 - accuracy: 0.4896\n",
            "Epoch 3/100\n",
            "96/96 - 0s - loss: 0.7215 - accuracy: 0.4844\n",
            "Epoch 4/100\n",
            "96/96 - 0s - loss: 0.7135 - accuracy: 0.5000\n",
            "Epoch 5/100\n",
            "96/96 - 0s - loss: 0.7071 - accuracy: 0.4948\n",
            "Epoch 6/100\n",
            "96/96 - 0s - loss: 0.7004 - accuracy: 0.5312\n",
            "Epoch 7/100\n",
            "96/96 - 0s - loss: 0.6961 - accuracy: 0.5312\n",
            "Epoch 8/100\n",
            "96/96 - 0s - loss: 0.6915 - accuracy: 0.5312\n",
            "Epoch 9/100\n",
            "96/96 - 0s - loss: 0.6908 - accuracy: 0.5365\n",
            "Epoch 10/100\n",
            "96/96 - 0s - loss: 0.6856 - accuracy: 0.5260\n",
            "Epoch 11/100\n",
            "96/96 - 0s - loss: 0.6830 - accuracy: 0.5312\n",
            "Epoch 12/100\n",
            "96/96 - 0s - loss: 0.6803 - accuracy: 0.5521\n",
            "Epoch 13/100\n",
            "96/96 - 0s - loss: 0.6778 - accuracy: 0.5417\n",
            "Epoch 14/100\n",
            "96/96 - 0s - loss: 0.6748 - accuracy: 0.5781\n",
            "Epoch 15/100\n",
            "96/96 - 0s - loss: 0.6724 - accuracy: 0.5885\n",
            "Epoch 16/100\n",
            "96/96 - 0s - loss: 0.6697 - accuracy: 0.5938\n",
            "Epoch 17/100\n",
            "96/96 - 0s - loss: 0.6675 - accuracy: 0.5938\n",
            "Epoch 18/100\n",
            "96/96 - 0s - loss: 0.6657 - accuracy: 0.6146\n",
            "Epoch 19/100\n",
            "96/96 - 0s - loss: 0.6628 - accuracy: 0.6250\n",
            "Epoch 20/100\n",
            "96/96 - 0s - loss: 0.6611 - accuracy: 0.6302\n",
            "Epoch 21/100\n",
            "96/96 - 0s - loss: 0.6589 - accuracy: 0.6250\n",
            "Epoch 22/100\n",
            "96/96 - 0s - loss: 0.6562 - accuracy: 0.6510\n",
            "Epoch 23/100\n",
            "96/96 - 0s - loss: 0.6537 - accuracy: 0.6406\n",
            "Epoch 24/100\n",
            "96/96 - 0s - loss: 0.6514 - accuracy: 0.6615\n",
            "Epoch 25/100\n",
            "96/96 - 0s - loss: 0.6502 - accuracy: 0.6510\n",
            "Epoch 26/100\n",
            "96/96 - 0s - loss: 0.6478 - accuracy: 0.6615\n",
            "Epoch 27/100\n",
            "96/96 - 0s - loss: 0.6456 - accuracy: 0.6615\n",
            "Epoch 28/100\n",
            "96/96 - 0s - loss: 0.6451 - accuracy: 0.6562\n",
            "Epoch 29/100\n",
            "96/96 - 0s - loss: 0.6420 - accuracy: 0.6667\n",
            "Epoch 30/100\n",
            "96/96 - 0s - loss: 0.6408 - accuracy: 0.6771\n",
            "Epoch 31/100\n",
            "96/96 - 0s - loss: 0.6386 - accuracy: 0.6719\n",
            "Epoch 32/100\n",
            "96/96 - 0s - loss: 0.6366 - accuracy: 0.6771\n",
            "Epoch 33/100\n",
            "96/96 - 0s - loss: 0.6362 - accuracy: 0.6719\n",
            "Epoch 34/100\n",
            "96/96 - 0s - loss: 0.6336 - accuracy: 0.6771\n",
            "Epoch 35/100\n",
            "96/96 - 0s - loss: 0.6313 - accuracy: 0.7083\n",
            "Epoch 36/100\n",
            "96/96 - 0s - loss: 0.6299 - accuracy: 0.7031\n",
            "Epoch 37/100\n",
            "96/96 - 0s - loss: 0.6272 - accuracy: 0.6979\n",
            "Epoch 38/100\n",
            "96/96 - 0s - loss: 0.6258 - accuracy: 0.6979\n",
            "Epoch 39/100\n",
            "96/96 - 0s - loss: 0.6232 - accuracy: 0.7031\n",
            "Epoch 40/100\n",
            "96/96 - 0s - loss: 0.6212 - accuracy: 0.7135\n",
            "Epoch 41/100\n",
            "96/96 - 0s - loss: 0.6224 - accuracy: 0.7031\n",
            "Epoch 42/100\n",
            "96/96 - 0s - loss: 0.6177 - accuracy: 0.7188\n",
            "Epoch 43/100\n",
            "96/96 - 0s - loss: 0.6158 - accuracy: 0.7188\n",
            "Epoch 44/100\n",
            "96/96 - 0s - loss: 0.6148 - accuracy: 0.7135\n",
            "Epoch 45/100\n",
            "96/96 - 0s - loss: 0.6121 - accuracy: 0.7292\n",
            "Epoch 46/100\n",
            "96/96 - 0s - loss: 0.6104 - accuracy: 0.7135\n",
            "Epoch 47/100\n",
            "96/96 - 0s - loss: 0.6089 - accuracy: 0.7292\n",
            "Epoch 48/100\n",
            "96/96 - 0s - loss: 0.6066 - accuracy: 0.7396\n",
            "Epoch 49/100\n",
            "96/96 - 0s - loss: 0.6044 - accuracy: 0.7188\n",
            "Epoch 50/100\n",
            "96/96 - 0s - loss: 0.6026 - accuracy: 0.7188\n",
            "Epoch 51/100\n",
            "96/96 - 0s - loss: 0.6008 - accuracy: 0.7240\n",
            "Epoch 52/100\n",
            "96/96 - 0s - loss: 0.5985 - accuracy: 0.7240\n",
            "Epoch 53/100\n",
            "96/96 - 0s - loss: 0.5967 - accuracy: 0.7240\n",
            "Epoch 54/100\n",
            "96/96 - 0s - loss: 0.5945 - accuracy: 0.7448\n",
            "Epoch 55/100\n",
            "96/96 - 0s - loss: 0.5923 - accuracy: 0.7344\n",
            "Epoch 56/100\n",
            "96/96 - 0s - loss: 0.5900 - accuracy: 0.7344\n",
            "Epoch 57/100\n",
            "96/96 - 0s - loss: 0.5880 - accuracy: 0.7396\n",
            "Epoch 58/100\n",
            "96/96 - 0s - loss: 0.5867 - accuracy: 0.7604\n",
            "Epoch 59/100\n",
            "96/96 - 0s - loss: 0.5839 - accuracy: 0.7500\n",
            "Epoch 60/100\n",
            "96/96 - 0s - loss: 0.5831 - accuracy: 0.7656\n",
            "Epoch 61/100\n",
            "96/96 - 0s - loss: 0.5801 - accuracy: 0.7552\n",
            "Epoch 62/100\n",
            "96/96 - 0s - loss: 0.5788 - accuracy: 0.7500\n",
            "Epoch 63/100\n",
            "96/96 - 0s - loss: 0.5764 - accuracy: 0.7708\n",
            "Epoch 64/100\n",
            "96/96 - 0s - loss: 0.5740 - accuracy: 0.7812\n",
            "Epoch 65/100\n",
            "96/96 - 0s - loss: 0.5722 - accuracy: 0.7812\n",
            "Epoch 66/100\n",
            "96/96 - 0s - loss: 0.5716 - accuracy: 0.7812\n",
            "Epoch 67/100\n",
            "96/96 - 0s - loss: 0.5687 - accuracy: 0.7865\n",
            "Epoch 68/100\n",
            "96/96 - 0s - loss: 0.5671 - accuracy: 0.7812\n",
            "Epoch 69/100\n",
            "96/96 - 0s - loss: 0.5647 - accuracy: 0.7760\n",
            "Epoch 70/100\n",
            "96/96 - 0s - loss: 0.5626 - accuracy: 0.7969\n",
            "Epoch 71/100\n",
            "96/96 - 0s - loss: 0.5604 - accuracy: 0.7917\n",
            "Epoch 72/100\n",
            "96/96 - 0s - loss: 0.5587 - accuracy: 0.7865\n",
            "Epoch 73/100\n",
            "96/96 - 0s - loss: 0.5559 - accuracy: 0.8073\n",
            "Epoch 74/100\n",
            "96/96 - 0s - loss: 0.5540 - accuracy: 0.8125\n",
            "Epoch 75/100\n",
            "96/96 - 0s - loss: 0.5517 - accuracy: 0.8073\n",
            "Epoch 76/100\n",
            "96/96 - 0s - loss: 0.5495 - accuracy: 0.8125\n",
            "Epoch 77/100\n",
            "96/96 - 0s - loss: 0.5476 - accuracy: 0.8125\n",
            "Epoch 78/100\n",
            "96/96 - 0s - loss: 0.5451 - accuracy: 0.8177\n",
            "Epoch 79/100\n",
            "96/96 - 0s - loss: 0.5428 - accuracy: 0.8229\n",
            "Epoch 80/100\n",
            "96/96 - 0s - loss: 0.5404 - accuracy: 0.8229\n",
            "Epoch 81/100\n",
            "96/96 - 0s - loss: 0.5390 - accuracy: 0.8125\n",
            "Epoch 82/100\n",
            "96/96 - 0s - loss: 0.5365 - accuracy: 0.8385\n",
            "Epoch 83/100\n",
            "96/96 - 0s - loss: 0.5346 - accuracy: 0.8542\n",
            "Epoch 84/100\n",
            "96/96 - 0s - loss: 0.5317 - accuracy: 0.8333\n",
            "Epoch 85/100\n",
            "96/96 - 0s - loss: 0.5304 - accuracy: 0.8385\n",
            "Epoch 86/100\n",
            "96/96 - 0s - loss: 0.5277 - accuracy: 0.8177\n",
            "Epoch 87/100\n",
            "96/96 - 0s - loss: 0.5255 - accuracy: 0.8125\n",
            "Epoch 88/100\n",
            "96/96 - 0s - loss: 0.5267 - accuracy: 0.8281\n",
            "Epoch 89/100\n",
            "96/96 - 0s - loss: 0.5232 - accuracy: 0.8125\n",
            "Epoch 90/100\n",
            "96/96 - 0s - loss: 0.5199 - accuracy: 0.8073\n",
            "Epoch 91/100\n",
            "96/96 - 0s - loss: 0.5179 - accuracy: 0.8125\n",
            "Epoch 92/100\n",
            "96/96 - 0s - loss: 0.5157 - accuracy: 0.8385\n",
            "Epoch 93/100\n",
            "96/96 - 0s - loss: 0.5154 - accuracy: 0.8438\n",
            "Epoch 94/100\n",
            "96/96 - 0s - loss: 0.5119 - accuracy: 0.8177\n",
            "Epoch 95/100\n",
            "96/96 - 0s - loss: 0.5100 - accuracy: 0.8281\n",
            "Epoch 96/100\n",
            "96/96 - 0s - loss: 0.5081 - accuracy: 0.8333\n",
            "Epoch 97/100\n",
            "96/96 - 0s - loss: 0.5070 - accuracy: 0.8490\n",
            "Epoch 98/100\n",
            "96/96 - 0s - loss: 0.5049 - accuracy: 0.8438\n",
            "Epoch 99/100\n",
            "96/96 - 0s - loss: 0.5033 - accuracy: 0.8646\n",
            "Epoch 100/100\n",
            "96/96 - 0s - loss: 0.5005 - accuracy: 0.8438\n",
            "32/1 - 0s - loss: 0.6311 - accuracy: 0.6562\n",
            "processing fold # 2\n",
            "Train on 96 samples\n",
            "Epoch 1/100\n",
            "96/96 - 1s - loss: 0.7034 - accuracy: 0.5260\n",
            "Epoch 2/100\n",
            "96/96 - 0s - loss: 0.6992 - accuracy: 0.5052\n",
            "Epoch 3/100\n",
            "96/96 - 0s - loss: 0.6952 - accuracy: 0.4948\n",
            "Epoch 4/100\n",
            "96/96 - 0s - loss: 0.6923 - accuracy: 0.5312\n",
            "Epoch 5/100\n",
            "96/96 - 0s - loss: 0.6902 - accuracy: 0.5208\n",
            "Epoch 6/100\n",
            "96/96 - 0s - loss: 0.6873 - accuracy: 0.5365\n",
            "Epoch 7/100\n",
            "96/96 - 0s - loss: 0.6856 - accuracy: 0.5469\n",
            "Epoch 8/100\n",
            "96/96 - 0s - loss: 0.6851 - accuracy: 0.5260\n",
            "Epoch 9/100\n",
            "96/96 - 0s - loss: 0.6825 - accuracy: 0.5417\n",
            "Epoch 10/100\n",
            "96/96 - 0s - loss: 0.6810 - accuracy: 0.5365\n",
            "Epoch 11/100\n",
            "96/96 - 0s - loss: 0.6795 - accuracy: 0.5417\n",
            "Epoch 12/100\n",
            "96/96 - 0s - loss: 0.6779 - accuracy: 0.5521\n",
            "Epoch 13/100\n",
            "96/96 - 0s - loss: 0.6764 - accuracy: 0.5573\n",
            "Epoch 14/100\n",
            "96/96 - 0s - loss: 0.6747 - accuracy: 0.5625\n",
            "Epoch 15/100\n",
            "96/96 - 0s - loss: 0.6733 - accuracy: 0.5677\n",
            "Epoch 16/100\n",
            "96/96 - 0s - loss: 0.6719 - accuracy: 0.5781\n",
            "Epoch 17/100\n",
            "96/96 - 0s - loss: 0.6729 - accuracy: 0.5729\n",
            "Epoch 18/100\n",
            "96/96 - 0s - loss: 0.6693 - accuracy: 0.5885\n",
            "Epoch 19/100\n",
            "96/96 - 0s - loss: 0.6679 - accuracy: 0.5885\n",
            "Epoch 20/100\n",
            "96/96 - 0s - loss: 0.6666 - accuracy: 0.5938\n",
            "Epoch 21/100\n",
            "96/96 - 0s - loss: 0.6652 - accuracy: 0.5990\n",
            "Epoch 22/100\n",
            "96/96 - 0s - loss: 0.6638 - accuracy: 0.6042\n",
            "Epoch 23/100\n",
            "96/96 - 0s - loss: 0.6638 - accuracy: 0.6198\n",
            "Epoch 24/100\n",
            "96/96 - 0s - loss: 0.6611 - accuracy: 0.6250\n",
            "Epoch 25/100\n",
            "96/96 - 0s - loss: 0.6599 - accuracy: 0.6354\n",
            "Epoch 26/100\n",
            "96/96 - 0s - loss: 0.6584 - accuracy: 0.6302\n",
            "Epoch 27/100\n",
            "96/96 - 0s - loss: 0.6567 - accuracy: 0.6406\n",
            "Epoch 28/100\n",
            "96/96 - 0s - loss: 0.6553 - accuracy: 0.6458\n",
            "Epoch 29/100\n",
            "96/96 - 0s - loss: 0.6538 - accuracy: 0.6406\n",
            "Epoch 30/100\n",
            "96/96 - 0s - loss: 0.6530 - accuracy: 0.6510\n",
            "Epoch 31/100\n",
            "96/96 - 0s - loss: 0.6511 - accuracy: 0.6562\n",
            "Epoch 32/100\n",
            "96/96 - 0s - loss: 0.6495 - accuracy: 0.6667\n",
            "Epoch 33/100\n",
            "96/96 - 0s - loss: 0.6482 - accuracy: 0.6771\n",
            "Epoch 34/100\n",
            "96/96 - 0s - loss: 0.6470 - accuracy: 0.6771\n",
            "Epoch 35/100\n",
            "96/96 - 0s - loss: 0.6448 - accuracy: 0.6823\n",
            "Epoch 36/100\n",
            "96/96 - 0s - loss: 0.6428 - accuracy: 0.6875\n",
            "Epoch 37/100\n",
            "96/96 - 0s - loss: 0.6410 - accuracy: 0.6979\n",
            "Epoch 38/100\n",
            "96/96 - 0s - loss: 0.6393 - accuracy: 0.6979\n",
            "Epoch 39/100\n",
            "96/96 - 0s - loss: 0.6378 - accuracy: 0.7083\n",
            "Epoch 40/100\n",
            "96/96 - 0s - loss: 0.6360 - accuracy: 0.7083\n",
            "Epoch 41/100\n",
            "96/96 - 0s - loss: 0.6341 - accuracy: 0.6875\n",
            "Epoch 42/100\n",
            "96/96 - 0s - loss: 0.6326 - accuracy: 0.7240\n",
            "Epoch 43/100\n",
            "96/96 - 0s - loss: 0.6314 - accuracy: 0.6927\n",
            "Epoch 44/100\n",
            "96/96 - 0s - loss: 0.6292 - accuracy: 0.7083\n",
            "Epoch 45/100\n",
            "96/96 - 0s - loss: 0.6272 - accuracy: 0.7240\n",
            "Epoch 46/100\n",
            "96/96 - 0s - loss: 0.6252 - accuracy: 0.7188\n",
            "Epoch 47/100\n",
            "96/96 - 0s - loss: 0.6233 - accuracy: 0.7240\n",
            "Epoch 48/100\n",
            "96/96 - 0s - loss: 0.6222 - accuracy: 0.7188\n",
            "Epoch 49/100\n",
            "96/96 - 0s - loss: 0.6201 - accuracy: 0.7135\n",
            "Epoch 50/100\n",
            "96/96 - 0s - loss: 0.6177 - accuracy: 0.7188\n",
            "Epoch 51/100\n",
            "96/96 - 0s - loss: 0.6185 - accuracy: 0.6979\n",
            "Epoch 52/100\n",
            "96/96 - 0s - loss: 0.6165 - accuracy: 0.7240\n",
            "Epoch 53/100\n",
            "96/96 - 0s - loss: 0.6123 - accuracy: 0.7396\n",
            "Epoch 54/100\n",
            "96/96 - 0s - loss: 0.6110 - accuracy: 0.7448\n",
            "Epoch 55/100\n",
            "96/96 - 0s - loss: 0.6090 - accuracy: 0.7448\n",
            "Epoch 56/100\n",
            "96/96 - 0s - loss: 0.6076 - accuracy: 0.7396\n",
            "Epoch 57/100\n",
            "96/96 - 0s - loss: 0.6066 - accuracy: 0.7448\n",
            "Epoch 58/100\n",
            "96/96 - 0s - loss: 0.6032 - accuracy: 0.7760\n",
            "Epoch 59/100\n",
            "96/96 - 0s - loss: 0.6007 - accuracy: 0.7604\n",
            "Epoch 60/100\n",
            "96/96 - 0s - loss: 0.5987 - accuracy: 0.7708\n",
            "Epoch 61/100\n",
            "96/96 - 0s - loss: 0.5966 - accuracy: 0.7812\n",
            "Epoch 62/100\n",
            "96/96 - 0s - loss: 0.5939 - accuracy: 0.7656\n",
            "Epoch 63/100\n",
            "96/96 - 0s - loss: 0.5924 - accuracy: 0.7604\n",
            "Epoch 64/100\n",
            "96/96 - 0s - loss: 0.5900 - accuracy: 0.7760\n",
            "Epoch 65/100\n",
            "96/96 - 0s - loss: 0.5880 - accuracy: 0.7812\n",
            "Epoch 66/100\n",
            "96/96 - 0s - loss: 0.5857 - accuracy: 0.7656\n",
            "Epoch 67/100\n",
            "96/96 - 0s - loss: 0.5847 - accuracy: 0.7812\n",
            "Epoch 68/100\n",
            "96/96 - 0s - loss: 0.5816 - accuracy: 0.7760\n",
            "Epoch 69/100\n",
            "96/96 - 0s - loss: 0.5806 - accuracy: 0.7812\n",
            "Epoch 70/100\n",
            "96/96 - 0s - loss: 0.5783 - accuracy: 0.7604\n",
            "Epoch 71/100\n",
            "96/96 - 0s - loss: 0.5758 - accuracy: 0.7708\n",
            "Epoch 72/100\n",
            "96/96 - 0s - loss: 0.5740 - accuracy: 0.7812\n",
            "Epoch 73/100\n",
            "96/96 - 0s - loss: 0.5720 - accuracy: 0.7865\n",
            "Epoch 74/100\n",
            "96/96 - 0s - loss: 0.5698 - accuracy: 0.7917\n",
            "Epoch 75/100\n",
            "96/96 - 0s - loss: 0.5681 - accuracy: 0.7917\n",
            "Epoch 76/100\n",
            "96/96 - 0s - loss: 0.5658 - accuracy: 0.7917\n",
            "Epoch 77/100\n",
            "96/96 - 0s - loss: 0.5687 - accuracy: 0.7812\n",
            "Epoch 78/100\n",
            "96/96 - 0s - loss: 0.5625 - accuracy: 0.7865\n",
            "Epoch 79/100\n",
            "96/96 - 0s - loss: 0.5609 - accuracy: 0.7969\n",
            "Epoch 80/100\n",
            "96/96 - 0s - loss: 0.5591 - accuracy: 0.7917\n",
            "Epoch 81/100\n",
            "96/96 - 0s - loss: 0.5573 - accuracy: 0.7917\n",
            "Epoch 82/100\n",
            "96/96 - 0s - loss: 0.5555 - accuracy: 0.7917\n",
            "Epoch 83/100\n",
            "96/96 - 0s - loss: 0.5537 - accuracy: 0.7969\n",
            "Epoch 84/100\n",
            "96/96 - 0s - loss: 0.5525 - accuracy: 0.7917\n",
            "Epoch 85/100\n",
            "96/96 - 0s - loss: 0.5499 - accuracy: 0.8073\n",
            "Epoch 86/100\n",
            "96/96 - 0s - loss: 0.5485 - accuracy: 0.8021\n",
            "Epoch 87/100\n",
            "96/96 - 0s - loss: 0.5461 - accuracy: 0.7969\n",
            "Epoch 88/100\n",
            "96/96 - 0s - loss: 0.5443 - accuracy: 0.8073\n",
            "Epoch 89/100\n",
            "96/96 - 0s - loss: 0.5425 - accuracy: 0.8021\n",
            "Epoch 90/100\n",
            "96/96 - 0s - loss: 0.5403 - accuracy: 0.7969\n",
            "Epoch 91/100\n",
            "96/96 - 0s - loss: 0.5384 - accuracy: 0.8073\n",
            "Epoch 92/100\n",
            "96/96 - 0s - loss: 0.5382 - accuracy: 0.8021\n",
            "Epoch 93/100\n",
            "96/96 - 0s - loss: 0.5349 - accuracy: 0.8073\n",
            "Epoch 94/100\n",
            "96/96 - 0s - loss: 0.5328 - accuracy: 0.8021\n",
            "Epoch 95/100\n",
            "96/96 - 0s - loss: 0.5337 - accuracy: 0.8021\n",
            "Epoch 96/100\n",
            "96/96 - 0s - loss: 0.5299 - accuracy: 0.8073\n",
            "Epoch 97/100\n",
            "96/96 - 0s - loss: 0.5279 - accuracy: 0.8073\n",
            "Epoch 98/100\n",
            "96/96 - 0s - loss: 0.5268 - accuracy: 0.8073\n",
            "Epoch 99/100\n",
            "96/96 - 0s - loss: 0.5263 - accuracy: 0.8073\n",
            "Epoch 100/100\n",
            "96/96 - 0s - loss: 0.5228 - accuracy: 0.8073\n",
            "32/1 - 0s - loss: 0.5269 - accuracy: 0.8281\n",
            "processing fold # 3\n",
            "Train on 96 samples\n",
            "Epoch 1/100\n",
            "96/96 - 0s - loss: 0.6994 - accuracy: 0.5208\n",
            "Epoch 2/100\n",
            "96/96 - 0s - loss: 0.6878 - accuracy: 0.5365\n",
            "Epoch 3/100\n",
            "96/96 - 0s - loss: 0.6821 - accuracy: 0.5833\n",
            "Epoch 4/100\n",
            "96/96 - 0s - loss: 0.6776 - accuracy: 0.6094\n",
            "Epoch 5/100\n",
            "96/96 - 0s - loss: 0.6739 - accuracy: 0.5833\n",
            "Epoch 6/100\n",
            "96/96 - 0s - loss: 0.6701 - accuracy: 0.5885\n",
            "Epoch 7/100\n",
            "96/96 - 0s - loss: 0.6669 - accuracy: 0.5938\n",
            "Epoch 8/100\n",
            "96/96 - 0s - loss: 0.6635 - accuracy: 0.5677\n",
            "Epoch 9/100\n",
            "96/96 - 0s - loss: 0.6619 - accuracy: 0.5729\n",
            "Epoch 10/100\n",
            "96/96 - 0s - loss: 0.6582 - accuracy: 0.5573\n",
            "Epoch 11/100\n",
            "96/96 - 0s - loss: 0.6556 - accuracy: 0.5729\n",
            "Epoch 12/100\n",
            "96/96 - 0s - loss: 0.6536 - accuracy: 0.6146\n",
            "Epoch 13/100\n",
            "96/96 - 0s - loss: 0.6506 - accuracy: 0.6406\n",
            "Epoch 14/100\n",
            "96/96 - 0s - loss: 0.6484 - accuracy: 0.6354\n",
            "Epoch 15/100\n",
            "96/96 - 0s - loss: 0.6468 - accuracy: 0.6094\n",
            "Epoch 16/100\n",
            "96/96 - 0s - loss: 0.6439 - accuracy: 0.6302\n",
            "Epoch 17/100\n",
            "96/96 - 0s - loss: 0.6411 - accuracy: 0.6198\n",
            "Epoch 18/100\n",
            "96/96 - 0s - loss: 0.6386 - accuracy: 0.6250\n",
            "Epoch 19/100\n",
            "96/96 - 0s - loss: 0.6367 - accuracy: 0.6302\n",
            "Epoch 20/100\n",
            "96/96 - 0s - loss: 0.6337 - accuracy: 0.6302\n",
            "Epoch 21/100\n",
            "96/96 - 0s - loss: 0.6316 - accuracy: 0.6250\n",
            "Epoch 22/100\n",
            "96/96 - 0s - loss: 0.6291 - accuracy: 0.6458\n",
            "Epoch 23/100\n",
            "96/96 - 0s - loss: 0.6265 - accuracy: 0.6510\n",
            "Epoch 24/100\n",
            "96/96 - 0s - loss: 0.6245 - accuracy: 0.6458\n",
            "Epoch 25/100\n",
            "96/96 - 0s - loss: 0.6208 - accuracy: 0.6406\n",
            "Epoch 26/100\n",
            "96/96 - 0s - loss: 0.6184 - accuracy: 0.6562\n",
            "Epoch 27/100\n",
            "96/96 - 0s - loss: 0.6218 - accuracy: 0.6198\n",
            "Epoch 28/100\n",
            "96/96 - 0s - loss: 0.6144 - accuracy: 0.6562\n",
            "Epoch 29/100\n",
            "96/96 - 0s - loss: 0.6113 - accuracy: 0.6458\n",
            "Epoch 30/100\n",
            "96/96 - 0s - loss: 0.6105 - accuracy: 0.6458\n",
            "Epoch 31/100\n",
            "96/96 - 0s - loss: 0.6078 - accuracy: 0.6510\n",
            "Epoch 32/100\n",
            "96/96 - 0s - loss: 0.6062 - accuracy: 0.6667\n",
            "Epoch 33/100\n",
            "96/96 - 0s - loss: 0.6038 - accuracy: 0.6562\n",
            "Epoch 34/100\n",
            "96/96 - 0s - loss: 0.6036 - accuracy: 0.6615\n",
            "Epoch 35/100\n",
            "96/96 - 0s - loss: 0.6013 - accuracy: 0.6510\n",
            "Epoch 36/100\n",
            "96/96 - 0s - loss: 0.5986 - accuracy: 0.6406\n",
            "Epoch 37/100\n",
            "96/96 - 0s - loss: 0.5961 - accuracy: 0.6667\n",
            "Epoch 38/100\n",
            "96/96 - 0s - loss: 0.5954 - accuracy: 0.6719\n",
            "Epoch 39/100\n",
            "96/96 - 0s - loss: 0.5924 - accuracy: 0.6719\n",
            "Epoch 40/100\n",
            "96/96 - 0s - loss: 0.5900 - accuracy: 0.6771\n",
            "Epoch 41/100\n",
            "96/96 - 0s - loss: 0.5877 - accuracy: 0.6823\n",
            "Epoch 42/100\n",
            "96/96 - 0s - loss: 0.5864 - accuracy: 0.6875\n",
            "Epoch 43/100\n",
            "96/96 - 0s - loss: 0.5837 - accuracy: 0.6823\n",
            "Epoch 44/100\n",
            "96/96 - 0s - loss: 0.5812 - accuracy: 0.6875\n",
            "Epoch 45/100\n",
            "96/96 - 0s - loss: 0.5811 - accuracy: 0.6979\n",
            "Epoch 46/100\n",
            "96/96 - 0s - loss: 0.5772 - accuracy: 0.6875\n",
            "Epoch 47/100\n",
            "96/96 - 0s - loss: 0.5753 - accuracy: 0.6979\n",
            "Epoch 48/100\n",
            "96/96 - 0s - loss: 0.5732 - accuracy: 0.7031\n",
            "Epoch 49/100\n",
            "96/96 - 0s - loss: 0.5715 - accuracy: 0.7031\n",
            "Epoch 50/100\n",
            "96/96 - 0s - loss: 0.5685 - accuracy: 0.6927\n",
            "Epoch 51/100\n",
            "96/96 - 0s - loss: 0.5661 - accuracy: 0.7083\n",
            "Epoch 52/100\n",
            "96/96 - 0s - loss: 0.5641 - accuracy: 0.6979\n",
            "Epoch 53/100\n",
            "96/96 - 0s - loss: 0.5620 - accuracy: 0.7031\n",
            "Epoch 54/100\n",
            "96/96 - 0s - loss: 0.5594 - accuracy: 0.7135\n",
            "Epoch 55/100\n",
            "96/96 - 0s - loss: 0.5568 - accuracy: 0.7240\n",
            "Epoch 56/100\n",
            "96/96 - 0s - loss: 0.5544 - accuracy: 0.7188\n",
            "Epoch 57/100\n",
            "96/96 - 0s - loss: 0.5519 - accuracy: 0.7188\n",
            "Epoch 58/100\n",
            "96/96 - 0s - loss: 0.5504 - accuracy: 0.7188\n",
            "Epoch 59/100\n",
            "96/96 - 0s - loss: 0.5473 - accuracy: 0.7344\n",
            "Epoch 60/100\n",
            "96/96 - 0s - loss: 0.5446 - accuracy: 0.7292\n",
            "Epoch 61/100\n",
            "96/96 - 0s - loss: 0.5422 - accuracy: 0.7344\n",
            "Epoch 62/100\n",
            "96/96 - 0s - loss: 0.5406 - accuracy: 0.7396\n",
            "Epoch 63/100\n",
            "96/96 - 0s - loss: 0.5389 - accuracy: 0.7396\n",
            "Epoch 64/100\n",
            "96/96 - 0s - loss: 0.5367 - accuracy: 0.7604\n",
            "Epoch 65/100\n",
            "96/96 - 0s - loss: 0.5358 - accuracy: 0.7448\n",
            "Epoch 66/100\n",
            "96/96 - 0s - loss: 0.5330 - accuracy: 0.7604\n",
            "Epoch 67/100\n",
            "96/96 - 0s - loss: 0.5320 - accuracy: 0.7604\n",
            "Epoch 68/100\n",
            "96/96 - 0s - loss: 0.5288 - accuracy: 0.7656\n",
            "Epoch 69/100\n",
            "96/96 - 0s - loss: 0.5266 - accuracy: 0.7552\n",
            "Epoch 70/100\n",
            "96/96 - 0s - loss: 0.5247 - accuracy: 0.7500\n",
            "Epoch 71/100\n",
            "96/96 - 0s - loss: 0.5250 - accuracy: 0.7500\n",
            "Epoch 72/100\n",
            "96/96 - 0s - loss: 0.5216 - accuracy: 0.7760\n",
            "Epoch 73/100\n",
            "96/96 - 0s - loss: 0.5198 - accuracy: 0.7865\n",
            "Epoch 74/100\n",
            "96/96 - 0s - loss: 0.5177 - accuracy: 0.7604\n",
            "Epoch 75/100\n",
            "96/96 - 0s - loss: 0.5159 - accuracy: 0.7552\n",
            "Epoch 76/100\n",
            "96/96 - 0s - loss: 0.5140 - accuracy: 0.7812\n",
            "Epoch 77/100\n",
            "96/96 - 0s - loss: 0.5160 - accuracy: 0.7708\n",
            "Epoch 78/100\n",
            "96/96 - 0s - loss: 0.5113 - accuracy: 0.7812\n",
            "Epoch 79/100\n",
            "96/96 - 0s - loss: 0.5099 - accuracy: 0.7812\n",
            "Epoch 80/100\n",
            "96/96 - 0s - loss: 0.5075 - accuracy: 0.8021\n",
            "Epoch 81/100\n",
            "96/96 - 0s - loss: 0.5053 - accuracy: 0.7917\n",
            "Epoch 82/100\n",
            "96/96 - 0s - loss: 0.5048 - accuracy: 0.7969\n",
            "Epoch 83/100\n",
            "96/96 - 0s - loss: 0.5033 - accuracy: 0.7865\n",
            "Epoch 84/100\n",
            "96/96 - 0s - loss: 0.5009 - accuracy: 0.8073\n",
            "Epoch 85/100\n",
            "96/96 - 0s - loss: 0.4996 - accuracy: 0.8125\n",
            "Epoch 86/100\n",
            "96/96 - 0s - loss: 0.4975 - accuracy: 0.7969\n",
            "Epoch 87/100\n",
            "96/96 - 0s - loss: 0.4963 - accuracy: 0.8073\n",
            "Epoch 88/100\n",
            "96/96 - 0s - loss: 0.4941 - accuracy: 0.8385\n",
            "Epoch 89/100\n",
            "96/96 - 0s - loss: 0.4924 - accuracy: 0.8125\n",
            "Epoch 90/100\n",
            "96/96 - 0s - loss: 0.4905 - accuracy: 0.8385\n",
            "Epoch 91/100\n",
            "96/96 - 0s - loss: 0.4886 - accuracy: 0.8177\n",
            "Epoch 92/100\n",
            "96/96 - 0s - loss: 0.4875 - accuracy: 0.8073\n",
            "Epoch 93/100\n",
            "96/96 - 0s - loss: 0.4853 - accuracy: 0.8333\n",
            "Epoch 94/100\n",
            "96/96 - 0s - loss: 0.4833 - accuracy: 0.8281\n",
            "Epoch 95/100\n",
            "96/96 - 0s - loss: 0.4831 - accuracy: 0.8281\n",
            "Epoch 96/100\n",
            "96/96 - 0s - loss: 0.4801 - accuracy: 0.8125\n",
            "Epoch 97/100\n",
            "96/96 - 0s - loss: 0.4785 - accuracy: 0.8281\n",
            "Epoch 98/100\n",
            "96/96 - 0s - loss: 0.4766 - accuracy: 0.8281\n",
            "Epoch 99/100\n",
            "96/96 - 0s - loss: 0.4769 - accuracy: 0.8177\n",
            "Epoch 100/100\n",
            "96/96 - 0s - loss: 0.4733 - accuracy: 0.8281\n",
            "32/1 - 0s - loss: 0.5096 - accuracy: 0.7969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_1hPktVJ5xD",
        "colab_type": "code",
        "outputId": "c3b608f3-b78c-4c19-b22e-65f1ce960507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.mean(l)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5760966241359711"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4__iudJ2zM3O",
        "colab_type": "code",
        "outputId": "050f4279-edce-4259-c5cd-8fe260b15804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "# prediciting the model\n",
        "for i in range(20,30):\n",
        "    prediction =  model.predict_classes(x_test[i].reshape(1,60))\n",
        "    prediction\n",
        "    if prediction == 0:\n",
        "      print(\"Prediction:\", prediction, \"-----> Mountain\")\n",
        "      print(\"Actual label :\",y_test[i])\n",
        "\n",
        "    else:\n",
        "      print(\"Prediction:\", prediction, \"-----> Rock\")\n",
        "      print(\"Actual label :\",y_test[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [1] -----> Rock\n",
            "Actual label : [1. 0.]\n",
            "Prediction: [0] -----> Mountain\n",
            "Actual label : [1. 0.]\n",
            "Prediction: [1] -----> Rock\n",
            "Actual label : [0. 1.]\n",
            "Prediction: [0] -----> Mountain\n",
            "Actual label : [1. 0.]\n",
            "Prediction: [0] -----> Mountain\n",
            "Actual label : [1. 0.]\n",
            "Prediction: [0] -----> Mountain\n",
            "Actual label : [1. 0.]\n",
            "Prediction: [1] -----> Rock\n",
            "Actual label : [0. 1.]\n",
            "Prediction: [0] -----> Mountain\n",
            "Actual label : [0. 1.]\n",
            "Prediction: [0] -----> Mountain\n",
            "Actual label : [0. 1.]\n",
            "Prediction: [1] -----> Rock\n",
            "Actual label : [0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AvcjSUs1IDl",
        "colab_type": "code",
        "outputId": "e2ff2202-cde3-4e22-d36b-b5ff8114f101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# for i in range(0,80):\n",
        "#     prediction =  model.predict_classes(x_test[i].reshape(1,60))\n",
        "#     if prediction != y_test[i]:\n",
        "#         print(\"Prediction:\",prediction,\"------->\",\"Actual Label:\",y_test[i])\n",
        "\n",
        "import numpy as np\n",
        "z = 0\n",
        "for i in range(0,80):\n",
        "    prediction =  model.predict_classes(x_test[i].reshape(1,60))\n",
        "    a = np.where(y_test[i] == 1)\n",
        "    a = np.array(a)\n",
        "    if prediction != a.item():\n",
        "        z+=1\n",
        "        # print(\"Prediction:\",prediction,\"------->\",\"Actual Label:\",a.item())\n",
        "print(\"***No of Invalid Predictions:\", z,\"***\")        "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***No of Invalid Predictions: 22 ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COy7TdX2zNzS",
        "colab_type": "code",
        "outputId": "584669e3-dd22-4cf2-ae15-64c6cce3c63f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 16)                976       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 1,010\n",
            "Trainable params: 1,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy-gCWzQlstP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}